<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Ready Player Me Avatar Speech Viewer</title>
  <style>
    body { margin: 0; overflow: hidden; background: #000; }
    canvas { display: block; }
    #controls {
      position: fixed;
      top: 10px;
      left: 10px;
      background: rgba(0, 0, 0, 0.7);
      padding: 10px;
      border-radius: 5px;
      color: white;
      font-family: Arial, sans-serif;
    }
    .viseme-control {
      margin: 5px 0;
    }
    .viseme-control label {
      display: inline-block;
      width: 100px;
    }
    .viseme-control input {
      width: 100px;
    }
    #test-button {
      margin-top: 10px;
      padding: 5px 10px;
      background: #4CAF50;
      border: none;
      color: white;
      border-radius: 3px;
      cursor: pointer;
    }
    #speech-controls {
      margin-top: 15px;
      border-top: 1px solid #555;
      padding-top: 10px;
    }
    #speech-input {
      width: 200px;
      padding: 5px;
      margin-right: 5px;
    }
    #speak-button {
      padding: 5px 10px;
      background: #2196F3;
      border: none;
      color: white;
      border-radius: 3px;
      cursor: pointer;
    }
    button:hover {
      opacity: 0.9;
    }
  </style>
  <!-- Import TalkingHead library -->
  <script src="https://cdn.jsdelivr.net/gh/met4citizen/TalkingHead@v1.4.0/dist/TalkingHead.min.js"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "https://unpkg.com/three@0.150.1/build/three.module.js",
        "three/addons/": "https://unpkg.com/three@0.150.1/examples/jsm/"
      }
    }
  </script>
  <script type="module">
    import * as THREE from 'three';
    import { OrbitControls } from 'three/addons/controls/OrbitControls.js';
    import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';

    // Global variables accessible outside module scope
    window.avatarLoaded = false;
    window.visemeMap = {};
    window.commonVisemes = ['mouthOpen', 'mouthSmile'];
    window.isSpeaking = false;
    window.talkingHead = null;

    let scene, camera, renderer, avatar, mixer, clock;
    
    init();
    loadAvatar('./avatar.glb');

    function init() {
      scene = new THREE.Scene();
      camera = new THREE.PerspectiveCamera(20, window.innerWidth / window.innerHeight, 0.1, 100);
      camera.position.set(0, 1.5, 7);

      renderer = new THREE.WebGLRenderer({ antialias: true });
      renderer.setSize(window.innerWidth, window.innerHeight);
      document.body.appendChild(renderer.domElement);

      const light = new THREE.DirectionalLight(0xffffff, 1);
      light.position.set(1, 2, 3);
      scene.add(light);
      scene.add(new THREE.AmbientLight(0x404040));

      // Create OrbitControls but disable them
      const controls = new OrbitControls(camera, renderer.domElement);
      controls.enabled = false; // Disable mouse control

      clock = new THREE.Clock();
      
      console.log("Three.js scene initialized");
      
      // Initialize TalkingHead
      if (window.TalkingHead) {
        window.talkingHead = new TalkingHead({
          language: 'en',  // Default language English
          rate: 1.0,       // Speech rate
          pitch: .0,      // Speech pitch
          volume: 1.0,     // Speech volume
          audio: false,     // Enable audio by default
          intensity: 2,  // Increased intensity for more pronounced visemes
          smoothing: 0.5   // Smoothing factor for viseme transitions
        });
        console.log("TalkingHead initialized");
      } else {
        console.warn("TalkingHead library not found, speech functionality will be limited");
      }
      
      // Listen for browser audioContext resuming
      window.addEventListener('click', function() {
        if (window.talkingHead && window.talkingHead.audioContext && 
            window.talkingHead.audioContext.state === 'suspended') {
          window.talkingHead.audioContext.resume();
          console.log("Audio context resumed");
        }
      }, { once: true });
    }

    function createVisemeControls() {
      const controlsDiv = document.createElement('div');
      controlsDiv.id = 'controls';
      
      // Add sliders for each viseme
      window.commonVisemes.forEach(viseme => {
        const div = document.createElement('div');
        div.className = 'viseme-control';
        
        const label = document.createElement('label');
        label.textContent = viseme;
        
        const input = document.createElement('input');
        input.type = 'range';
        input.min = '0';
        input.max = '1';
        input.step = '0.1';
        input.value = '0';
        
        input.addEventListener('input', (e) => {
          setViseme(viseme, parseFloat(e.target.value));
        });
        
        div.appendChild(label);
        div.appendChild(input);
        controlsDiv.appendChild(div);
      });

      // Add test button
      const testButton = document.createElement('button');
      testButton.id = 'test-button';
      testButton.textContent = 'Test Visemes';
      testButton.addEventListener('click', testVisemes);
      controlsDiv.appendChild(testButton);

      // Add speech controls
      const speechControls = document.createElement('div');
      speechControls.id = 'speech-controls';
      
      const speechInput = document.createElement('input');
      speechInput.id = 'speech-input';
      speechInput.type = 'text';
      speechInput.placeholder = 'Enter text to speak...';
      
      const speakButton = document.createElement('button');
      speakButton.id = 'speak-button';
      speakButton.textContent = 'Speak';
      speakButton.addEventListener('click', () => {
        const text = speechInput.value.trim();
        if (text) {
          speakText(text);
        }
      });
      
      speechControls.appendChild(speechInput);
      speechControls.appendChild(speakButton);
      controlsDiv.appendChild(speechControls);

      document.body.appendChild(controlsDiv);
    }

    function testVisemes() {
      console.log("Running test visemes sequence");
      
      const sequence = [
        { viseme: 'mouthOpen', duration: 500, value: 0.8 },
        { viseme: 'mouthOpen', duration: 300, value: 0.0 },
        { viseme: 'mouthSmile', duration: 500, value: 0.8 },
        { viseme: 'mouthSmile', duration: 300, value: 0.0 },
        { viseme: 'mouthOpen', duration: 400, value: 0.5 },
        { viseme: 'mouthOpen', duration: 300, value: 0.0 }
      ];

      let currentIndex = 0;
      
      function playNext() {
        if (currentIndex >= sequence.length) {
          // Reset all visemes
          window.commonVisemes.forEach(viseme => setViseme(viseme, 0));
          console.log("Test viseme sequence completed");
          return;
        }

        const { viseme, duration, value } = sequence[currentIndex];
        console.log(`Setting viseme: ${viseme} to ${value}`);
        
        // Set current viseme
        setViseme(viseme, value);
        
        currentIndex++;
        setTimeout(playNext, duration);
      }

      playNext();
    }

    function setViseme(visemeName, weight) {
      if (!avatar) {
        console.log(`Cannot set viseme ${visemeName}: avatar not loaded`);
        return;
      }
      
      console.log(`Setting viseme ${visemeName} to ${weight}`);
      
      avatar.traverse(child => {
        if (child.isMesh && child.morphTargetInfluences && window.visemeMap[visemeName] !== undefined) {
          child.morphTargetInfluences[window.visemeMap[visemeName]] = weight;
        }
      });
    }

    // Map phonemes to visemes - simplified mapping based on TalkingHead
    const phoneToVisemeMap = {
      'a': 'mouthOpen',
      'e': 'mouthOpen',
      'i': 'mouthSmile',
      'o': 'mouthOpen',
      'u': 'mouthOpen',
      'p': 'mouthOpen',
      'b': 'mouthOpen',
      'm': 'mouthOpen',
      'f': 'mouthOpen',
      'v': 'mouthOpen',
      't': 'mouthOpen',
      'd': 'mouthOpen',
      'n': 'mouthOpen',
      's': 'mouthSmile',
      'z': 'mouthSmile',
      'th': 'mouthOpen',
      'ch': 'mouthOpen',
      'j': 'mouthOpen',
      'sh': 'mouthSmile',
      'zh': 'mouthSmile',
      'k': 'mouthOpen',
      'g': 'mouthOpen',
      'ng': 'mouthOpen',
      'h': 'mouthOpen',
      'w': 'mouthOpen',
      'y': 'mouthSmile',
      'r': 'mouthOpen',
      'l': 'mouthOpen'
    };

    // Function to speak text using TalkingHead with audio synchronization
    function speakText(text) {
      if (window.isSpeaking) {
        console.log("Already speaking, please wait");
        return;
      }
      
      console.log(`Speaking: "${text}"`);
      window.isSpeaking = true;
      
      // Reset all visemes
      window.commonVisemes.forEach(viseme => setViseme(viseme, 0));
      
      if (window.talkingHead) {
        // Use TalkingHead library to process speech with audio
        window.talkingHead.speak(text, {
          audio: false,         // Enable audio generation
          rate: 1.0,           // Speech rate
          pitch: 1.0,          // Speech pitch
          volume: 1.0,         // Speech volume
          intensity: 1.5,      // Increased intensity for more pronounced visemes (default is 1.0)
          smoothing: 0.5,      // Smoothing factor for viseme transitions
          onstart: () => {
            console.log("Speech started");
            
            // Send event to Flutter
            try {
              if (window.flutter_inappwebview) {
                window.flutter_inappwebview.callHandler('avatarEvent', 'speakStart', { text });
              }
            } catch (e) {
              console.error("Error sending speak start event to Flutter:", e);
            }
          },
          onviseme: (viseme, value, time) => {
            // Map TalkingHead visemes to our viseme system with amplified values
            const visemeName = mapToAvatarViseme(viseme);
            if (visemeName) {
              // Amplify the value to make mouth movements more pronounced
              const amplifiedValue = Math.min(value * 1.75, 1.0);
              
              setTimeout(() => {
                setViseme(visemeName, amplifiedValue);
                
                // For certain strong visemes, add secondary movements
                if (viseme === 'A' || viseme === 'E' || viseme === 'O') {
                  // For vowels, add some smile when mouth is open for more expressiveness
                  if (visemeName === 'mouthOpen' && amplifiedValue > 0.5) {
                    setViseme('mouthSmile', amplifiedValue * 0.3);
                  }
                }
              }, time * 1000);
            }
          },
          onend: () => {
            console.log("Speech ended");
            window.isSpeaking = false;
            
            // Reset all visemes
            setTimeout(() => {
              window.commonVisemes.forEach(viseme => setViseme(viseme, 0));
            }, 100);
            
            // Send event to Flutter
            try {
              if (window.flutter_inappwebview) {
                window.flutter_inappwebview.callHandler('avatarEvent', 'speakEnd', { text });
              }
            } catch (e) {
              console.error("Error sending speak end event to Flutter:", e);
            }
          }
        });
      } else {
        // Fallback if TalkingHead is not available - simple animation
        console.log("Using fallback speech animation");
        simulateSpeech(text);
      }
    }

    // Map TalkingHead visemes to our avatar's visemes
    function mapToAvatarViseme(thViseme) {
      // Improved mapping with better viseme correlation
      const mapping = {
        'A': 'mouthOpen',    // 'ah' as in "father" - wide open mouth
        'B': 'mouthOpen',    // 'b', 'm', 'p' - closed lips that should pop open
        'C': 'mouthSmile',   // 'ee' as in "meet" - spread lips
        'D': 'mouthOpen',    // 'oh' as in "toe" - rounded lips
        'E': 'mouthOpen',    // 'ah' as in "bat" - slightly open mouth
        'F': 'mouthSmile',   // 'f', 'v' - lower lip against upper teeth
        'G': 'mouthSmile',   // dental 't', 'd', 'n' - tongue tip behind upper teeth
        'H': 'mouthOpen',    // 'k', 'g', 'ng' - tongue back raised
        'X': 'mouthOpen'     // Rest position - neutral/slight mouth opening
      };
      
      // If we don't have an exact match for the viseme, return the closest one
      // Prioritize mouthOpen for most speech sounds as it's more visible
      return mapping[thViseme] || 'mouthOpen';
    }

    // Simple fallback animation if TalkingHead is not available
    function simulateSpeech(text) {
      const words = text.split(' ');
      const duration = Math.max(1000, words.length * 300); // Rough estimate of duration
      const steps = Math.floor(duration / 100); // 10 frames per second
      
      // Try to create audio with the Web Speech API as a fallback
      let utterance = null;
      if (window.speechSynthesis) {
        utterance = new SpeechSynthesisUtterance(text);
        utterance.rate = 1.0;
        utterance.pitch = 1.0;
        utterance.volume = 1.0;
        window.speechSynthesis.speak(utterance);
      }
      
      // Send event to Flutter
      try {
        if (window.flutter_inappwebview) {
          window.flutter_inappwebview.callHandler('avatarEvent', 'speakStart', { text });
        }
      } catch (e) {
        console.error("Error sending speak start event to Flutter:", e);
      }
      
      let currentStep = 0;
      
      const interval = setInterval(() => {
        if (currentStep >= steps) {
          clearInterval(interval);
          window.commonVisemes.forEach(viseme => setViseme(viseme, 0));
          window.isSpeaking = false;
          
          // Send event to Flutter
          try {
            if (window.flutter_inappwebview) {
              window.flutter_inappwebview.callHandler('avatarEvent', 'speakEnd', { text });
            }
          } catch (e) {
            console.error("Error sending speak end event to Flutter:", e);
          }
          
          return;
        }
        
        // More realistic mouth movements with occasional pauses
        const phase = currentStep / steps;
        let openValue, smileValue;
        
        if (currentStep % 5 === 0) {
          // Occasional wider mouth opening for emphasis
          openValue = 0.7 + Math.random() * 0.3; // Between 0.7 and 1.0
          smileValue = Math.random() * 0.4;
        } else if (currentStep % 10 === 0) {
          // Occasional pause
          openValue = 0.1;
          smileValue = 0.1;
        } else {
          // Normal speaking movement
          openValue = 0.3 + Math.random() * 0.5; // Between 0.3 and 0.8
          smileValue = Math.random() * 0.3;
        }
        
        setViseme('mouthOpen', openValue);
        setViseme('mouthSmile', smileValue);
        
        currentStep++;
      }, 100);
    }

    function loadAvatar(url) {
      console.log(`Loading avatar from ${url}`);
      const loader = new GLTFLoader();
      loader.load(
        url,
        gltf => {
          console.log("Avatar loaded successfully");
          avatar = gltf.scene;
          avatar.scale.set(5, 5, 5);
          
          // Position the avatar lower on the screen
          avatar.position.y = -8.5; // Adjust this value as needed
          
          scene.add(avatar);

          // Get viseme blendshapes from skinned mesh
          avatar.traverse(child => {
            if (child.isMesh && child.morphTargetDictionary) {
              window.visemeMap = child.morphTargetDictionary;
              console.log('Available Visemes:', window.visemeMap);
              
              // If we have no visemes, add dummy ones for testing
              if (Object.keys(window.visemeMap).length === 0) {
                console.log("No visemes found in model, adding dummy ones");
                window.visemeMap = {
                  'mouthOpen': 0,
                  'mouthSmile': 1
                };
                
                // Add dummy morph targets
                if (child.morphTargetInfluences === undefined || child.morphTargetInfluences.length === 0) {
                  child.morphTargetInfluences = [0, 0];
                }
              }
            }
          });
          
          // Create the UI controls
          // createVisemeControls();
          
          // Set flag that avatar is loaded and ready
          window.avatarLoaded = true;
          
          // Send ready event to Flutter
          try {
            if (window.flutter_inappwebview) {
              window.flutter_inappwebview.callHandler('avatarEvent', 'ready', {
                visemes: Object.keys(window.visemeMap)
              });
            }
          } catch (e) {
            console.error("Error sending ready event to Flutter:", e);
          }

          animate();
        },
        xhr => {
          console.log((xhr.loaded / xhr.total * 100) + '% loaded');
        },
        error => {
          console.error('An error happened while loading the model:', error);
          
          // Create a fallback avatar for testing
          console.log("Creating fallback avatar for testing");
          const geometry = new THREE.BoxGeometry(1, 1, 1);
          const material = new THREE.MeshBasicMaterial({ color: 0xff0000 });
          avatar = new THREE.Mesh(geometry, material);
          scene.add(avatar);
          
          // Set dummy viseme map
          window.visemeMap = {
            'mouthOpen': 0,
            'mouthSmile': 1
          };
          
          // Create the UI controls
          // createVisemeControls();
          
          // Set flag that avatar is loaded
          window.avatarLoaded = true;
          
          animate();
        }
      );
    }

    function animate() {
      requestAnimationFrame(animate);
      renderer.render(scene, camera);
    }

    // Expose functions to window object for Flutter to call
    window.setViseme = setViseme;
    window.testVisemes = testVisemes;
    window.speakText = speakText;
    window.getVisemeList = () => Object.keys(window.visemeMap);
    window.resetVisemes = () => {
      window.commonVisemes.forEach(viseme => setViseme(viseme, 0));
    };
    
    // Simple ping function to test communication
    window.ping = () => {
      console.log("Ping received from Flutter!");
      return "pong";
    };
  </script>
</head>
<body>
</body>
</html>